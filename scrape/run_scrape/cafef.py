import sys
import os
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
# Th√™m th∆∞ m·ª•c g·ªëc project v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))
import sys
import json
import asyncio
from datetime import datetime
import asyncpg
from dotenv import load_dotenv
import os

from scrape.scrape_script.cafef import scrape_cafef_requests
from utils import save_articles_to_db

load_dotenv()

async def main(json_path):
    # 1. ƒê·ªçc JSON input
    with open(json_path, 'r', encoding='utf-8') as f:
        articles = json.load(f)

    enriched_articles = []
    for i, article in enumerate(articles):
        url = article.get("href")
        if not url:
            print(f"‚ö†Ô∏è B·ªè qua b√†i {i} v√¨ thi·∫øu href.")
            continue

        print(f"üîç [{i+1}/{len(articles)}] Scraping: {url}")
        try:
            content_data = scrape_cafef_requests(url) 
            if isinstance(content_data, dict):
                article.update(content_data)
            else:
                article["content"] = content_data

            enriched_articles.append(article)
            print(f"‚úÖ Scraped: {article['title']}")
        except Exception as e:
            print(f"‚ùå L·ªói scrape {url}: {e}")

    # 2. L∆∞u v√†o DB
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        print("‚ùå Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng DATABASE_URL")
        return

    try:
        pool = await asyncpg.create_pool(dsn=db_url)
        await save_articles_to_db(pool, enriched_articles)
    finally:
        await pool.close()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("‚ùå C·∫ßn truy·ªÅn ƒë∆∞·ªùng d·∫´n file JSON.")
        sys.exit(1)

    json_path = sys.argv[1]
    asyncio.run(main(json_path))
