import sys
import os
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
# Th√™m th∆∞ m·ª•c g·ªëc project v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))

import sys
import json
import asyncio
from datetime import datetime
import asyncpg
from dotenv import load_dotenv
import os

from scrape.scrape_script.vietstock import scrape_vietstock_playwright  # ‚úÖ ƒë√∫ng module b·∫°n d√πng
from utils import save_articles_to_db

load_dotenv()

async def main(json_path):
    # 1. ƒê·ªçc danh s√°ch b√†i vi·∫øt t·ª´ file JSON
    with open(json_path, 'r', encoding='utf-8') as f:
        articles = json.load(f)

    enriched_articles = []

    for i, article in enumerate(articles):
        url = article.get("href")
        if not url:
            print(f"‚ö†Ô∏è B·ªè qua b√†i {i} v√¨ thi·∫øu href.")
            continue

        print(f"üîç [{i+1}/{len(articles)}] Scraping: {url}")
        try:
            scraped = await scrape_vietstock_playwright(url)  # Tr·∫£ v·ªÅ list[dict]
            if scraped:
                enriched = scraped[0]
                article.update(enriched)
                enriched_articles.append(article)
                print(f"‚úÖ Scraped: {article['title']}")
            else:
                print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c n·ªôi dung: {url}")
        except Exception as e:
            print(f"‚ùå L·ªói scrape {url}: {e}")

    # 2. L∆∞u d·ªØ li·ªáu v√†o DB
    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        print("‚ùå Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng DATABASE_URL")
        return

    try:
        pool = await asyncpg.create_pool(dsn=db_url)
        await save_articles_to_db(pool, enriched_articles)
    finally:
        await pool.close()

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("‚ùå C·∫ßn truy·ªÅn ƒë∆∞·ªùng d·∫´n file JSON.")
        sys.exit(1)

    json_path = sys.argv[1]
    asyncio.run(main(json_path))
